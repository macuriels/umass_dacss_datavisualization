---
title: "What Is the State of the Job Market for Social Scientists and Data Analysts in the US in 2024?"
output: html_document
# date: "2024-02-27"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(flexdashboard)
library(ggplot2)
library(dplyr)
library(stringr)
library(plotly)
library(knitr)
library(DT)
library(shiny)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(lubridate)
library(maps)


jobpostings <- read.csv('jobpostings_dataset.csv')
```


## {.tabset .tabset-fade}

Click on each tab below to discover different insights by topic.

### Top Skills

```{r, echo=FALSE}
# Extract skills from rows that begin with "Skills:"
skills <- str_extract_all(jobpostings$Skill, "(?<=Skills: )(.*?)(?= \\+\\d+ more)")

# Flatten the list of skills and split by commas
skills <- unlist(skills)
skills <- unlist(strsplit(skills, ",\\s*"))

# Remove empty strings
skills <- skills[skills != ""]

# Create a dataframe with the counts of each skill
skill_counts <- table(skills) %>%
  as.data.frame() %>%
  setNames(c("Skill", "Count"))

# Remove the counts for rows that begin with "X of 10 skills match your profile - you may be a good fit"
skill_counts <- skill_counts[!grepl("^\\d+ of 10 skills match your profile - you may be a good fit", skill_counts$Skill), ]

# Filter the skill_counts data frame to include only the top 20 skills
top_skills <- head(skill_counts[order(-skill_counts$Count), ], 20)

# Determine the maximum count for the x-axis range
max_count <- max(top_skills$Count)

# Manually reorder the Skill factor
top_skills$Skill <- factor(top_skills$Skill, levels = rev(top_skills$Skill))

# Create a horizontal bar plot
ggplot(top_skills, aes(x = Skill, y = Count)) +
  geom_bar(stat = "identity", fill = "gray", width = 0.5) +
  geom_bar(data = subset(top_skills, 
                         Skill == "Project Management"), 
           stat = "identity", fill = "#00abff", width = 0.5) +
  geom_text(aes(label = ifelse(Skill == "Project Management", 
                               "Management is in the top 20 most common skills", "")),
            hjust = -0.1, vjust = 0.5, size = 3.5, 
            color = "#00abff", position = position_dodge(width = 0.9)) +
  labs(title = "Most Sought Skills for Social Scientists and Data Analysts",
       subtitle = "For US-only Jobs Posted on LinkedIn in 2024",
       caption = "Based on 1740 jobs posted between February 7 and April 18, 2024",
       x = "",
       y = "Number of job posts that have this skill") +
  theme(axis.text.y = element_text(size = 5),
        # panel.background = element_rect(fill = "white"),
        legend.position = "none") +
  coord_flip() +
  theme_classic()
```

### Salaries

```{r, echo=FALSE, warning=FALSE}
# Remove incorrect salaries
salaries_clean <- jobpostings[jobpostings$SalaryLowRange > 20000, ]

# Filter out rows with missing or non-numeric SalaryLowRange
salaries_clean <- salaries_clean %>%
  filter(!is.na(SalaryLowRange), is.numeric(SalaryLowRange))

# Calculate salary data points
lowest_salary <- min(salaries_clean$SalaryLowRange)
average_salary <- quantile(jobpostings$SalaryLowRange, 0.50, na.rm = TRUE)


# Create a box and whisker plot
ggplot(salaries_clean, aes(y = SalaryLowRange)) +
    geom_boxplot(fill = "#00abff", color = "gray", outlier.shape = 1) +
    geom_text(aes(label = paste("Salaries start at $", 
                                lowest_salary, sep = "")),
              x = Inf, y = Inf, vjust = 30, hjust = 2.5, 
            size = 3.5, color = "#00abff") +
    geom_text(aes(label = paste("and on average are around $", 
                                average_salary, sep = "")),
              x = Inf, y = Inf, vjust = 40, hjust = 2.0, 
            size = 3.0, color = "gray") +
  labs(title = "Starting Yearly Salary for Social Scientists and Data Analysts",
       subtitle = "For US-only Jobs Posted on LinkedIn in 2024",
       caption = "Based on 1740 jobs posted between February 7 and April 18, 2024",
       y = "Starting Salary", 
       x = "") +
  coord_flip() +
  theme_classic() +
  scale_y_continuous(labels = scales::number_format(scale = 1e-3,
                                                    prefix = "$",
                                                    suffix = "k"))
```

### Company Size

```{r, echo=FALSE, warning=FALSE}
# Define the fill colors
fill_colors <- c("#99ccff", "#66b3ff", "#00abff", "#0088ff")

# Convert EnterpriseSize to factor with custom order
jobpostings$EnterpriseSize <- factor(jobpostings$EnterpriseSize, 
                                          levels = c("Micro Enterprise"
                                                     , "Small Enterprise"
                                                     , "Medium Enterprise"
                                                     , "Large Enterprise"))

# Remove incorrect salaries
company_size <- jobpostings[jobpostings$SalaryLowRange > 20000, ]

# Remove company's with null size
company_size <- company_size %>%
  filter(!is.na(EnterpriseSize))

# Create a box and whisker plot
ggplot(company_size, aes(y = SalaryLowRange, fill = EnterpriseSize)) +
  geom_boxplot(outlier.shape = 1, color = "gray") +
  labs(title = "Starting Yearly Salary by Business Size",
       subtitle = "For US-only Jobs Posted on LinkedIn in 2024",
       caption = "Based on 1740 jobs posted between February 7 and April 18, 2024",
       y = "Starting Salary") +
  theme_classic() +
  scale_y_continuous(labels = scales::number_format(scale = 1e-3, 
                                                    prefix = "$",
                                                    suffix = "k")) +
  scale_fill_manual(values = fill_colors) +
  geom_text(aes(label = "Bigger companies tend to pay better"),
              x = Inf, y = Inf, vjust = 2.5, hjust = 1.0, 
            size = 3.0, color = "#00abff") 
  # coord_flip()
```


### Job Locations

```{r, echo=FALSE, warning=FALSE}
# Count and group job postings by state
state_counts <- jobpostings %>%
  mutate(State = str_extract(Location, "(?<=,)[^,]+(?=,)")) %>%
  group_by(State) %>%
  summarise(Count = n()) %>%
  filter(!is.na(State)) %>%
  mutate(State = tolower(State))

# Remove trailing and leading white space
state_counts$State <- trimws(state_counts$State)

# Load the US map data
us_map <- map_data("state")

# Merge the state map data with the job counts
merged_geo_data <- merge(us_map, state_counts, by.x = "region", by.y = "State", all.x = TRUE)

# Sort by group, then order
merged_geo_data <- arrange(merged_geo_data, group, order)

# Add two-letter state codes to merged_geo_data
merged_geo_data$state_code <- state.abb[match(merged_geo_data$region, tolower(state.name))]

# Calculate the center of each state for label placement
state_centers <- merged_geo_data %>%
  group_by(state_code) %>%
  summarize(long = mean(long), lat = mean(lat))

# Add a dummy 'group' column to state_centers
state_centers$group <- 1
state_centers$Count <- 1

# Create the choropleth map
ggplot(merged_geo_data, aes(x = long, y = lat, group = group, fill = Count)) +
  geom_polygon(color = "darkgray") +
  scale_fill_gradient(low = "lightblue", high = "blue", na.value = "gray") +
  labs(title = "Most Popular Job Locations by State",
       subtitle = "For US-only Jobs Posted on LinkedIn in 2024",
       caption = "Based on 1740 jobs posted between February 7 and April 18, 2024",
       fill = "Number of Job Postings") +
  theme_void() +
  theme(aspect.ratio = .75) +
  geom_text(data = state_centers
            , aes(x = long, y = lat, label = state_code)
            , size = 2.5
            , color = "black") +
  geom_text(aes(label = "California = where most jobs are HQ'ed"),
              x = Inf, y = Inf, vjust = 2.5, hjust = 1, 
            size = 3.0, color = "orange") 

```


### Methodology

Job openings posted on LinkedIn are manually extracted by using a Google Chrome Extension ([Jobs Scraper & Exporter for LinkedIn](https://chromewebstore.google.com/detail/jobs-scraper-for-linkedin/jhmlphenakpfjkieogpinommlgjdjnhb)).

This extension allows users to download the first 20 jobs that appear on LinkedIn for any given search, along with multiple data points, such as the date published and the job poster.

All jobs searched are in the United States and a plethora of key words related to social science and data analytics are used to find akin jobs.

Below is the list of key words that were used to find jobs:

-   Behavioral Economist

-   Behavioral Scientist

-   Data Analyst

-   Data Scientist

-   Human Factors

-   Insights Analyst

-   Marketing Analyst

-   People Analytics

-   Policy Analyst

-   Product Analyst

-   Social Scientist

-   UX Researcher

Each key word was searched on a Thursday (with a couple of exceptions of this occuring on Fridays), filtering only for jobs that were posted in the past week.

Upon downloading all of the job posts, multiple data wrangling operations were performed to clean and enhance the dataset.

The data wrangling operations were performed using R, and include deduplicating job postings, searching for salary information in the job description, and locating mentions of key words such as "artificial intelligence".

Below is a snippet of the resulting dataframe, which is also used to render all the plots and insights.

```{r, echo=FALSE}
# Render the table using datatable
sample_df <- jobpostings %>% slice_sample(n = 2)
datatable(sample_df)

# Alternative: Render the table using kable with HTML format
# kable(sample_df, format = "html")
```
